{
 "cells": [
  {
   "cell_type": "code",
   "id": "8b8c8dd27e740f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:55:08.894057Z",
     "start_time": "2025-02-03T15:55:07.843727Z"
    }
   },
   "source": [
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "from rdflib import Graph\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# 1. 加载 RDF 图数据\n",
    "def load_rdf_data():\n",
    "    graphs = []\n",
    "    try:\n",
    "        g1 = Graph()\n",
    "        g1.parse(\n",
    "            \"/Users/yingji/Library/CloudStorage/OneDrive-MicrosoftOffice365/VU_course/AI_master/ML for Graph/RDF2Vec/dataset/AIFB.n3\",\n",
    "            format=\"n3\")\n",
    "        graphs.append(g1)\n",
    "\n",
    "        # g2 = Graph()\n",
    "        # g2.parse(\n",
    "        #     \"/Users/yingji/Library/CloudStorage/OneDrive-MicrosoftOffice365/VU_course/AI_master/ML for Graph/RDF2Vec/dataset/BGS.nt\",\n",
    "        #     format=\"nt\")\n",
    "        # graphs.append(g2)\n",
    "        #\n",
    "        # g3 = Graph()\n",
    "        # g3.parse(\n",
    "        #     \"/Users/yingji/Library/CloudStorage/OneDrive-MicrosoftOffice365/VU_course/AI_master/ML for Graph/RDF2Vec/dataset/wikidata-20250117-lexeme-BETA.nt\",\n",
    "        #     format=\"nt\")\n",
    "        # graphs.append(g3)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing RDF files: {e}\")\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# 2. 使用图遍历生成路径序列 (广度优先搜索)\n",
    "def generate_graph_walks(graphs, depth=10):  # 增加默认深度\n",
    "    walks = []\n",
    "    for graph in graphs:\n",
    "        nx_graph = nx.DiGraph()\n",
    "        for subj, pred, obj in graph:\n",
    "            nx_graph.add_edge(str(subj), str(obj), label=str(pred))\n",
    "\n",
    "        visited = set()\n",
    "\n",
    "        # 遍历每个节点，确保所有节点被访问\n",
    "        for node in nx_graph.nodes:\n",
    "            if node not in visited:\n",
    "                queue = [(node, [node])]\n",
    "                visited.add(node)\n",
    "\n",
    "                for _ in range(depth):\n",
    "                    next_queue = []\n",
    "                    for current, path in queue:\n",
    "                        neighbors = list(nx_graph.neighbors(current))\n",
    "\n",
    "                        # 如果没有邻居，确保孤立节点也被记录\n",
    "                        if not neighbors and len(path) == 1:\n",
    "                            walks.append(path)\n",
    "\n",
    "                        for neighbor in neighbors:\n",
    "                            new_path = path + [neighbor]\n",
    "                            next_queue.append((neighbor, new_path))\n",
    "\n",
    "                            if len(new_path) == depth:\n",
    "                                walks.append(new_path)\n",
    "                            visited.add(neighbor)\n",
    "                    queue = next_queue\n",
    "\n",
    "        # 处理孤立节点（无入度和出度的节点）\n",
    "        isolated_nodes = set(nx_graph.nodes) - visited\n",
    "        for isolated_node in isolated_nodes:\n",
    "            walks.append([isolated_node])\n",
    "\n",
    "    return walks\n",
    "\n",
    "\n",
    "# 3. 使用 Weisfeiler-Lehman 子树算法生成路径\n",
    "def generate_wl_subtree_walks(graphs, iterations=4, depth=2):\n",
    "    walks = []\n",
    "    for graph in graphs:\n",
    "        nx_graph = nx.DiGraph()\n",
    "        for subj, pred, obj in graph:\n",
    "            nx_graph.add_edge(str(subj), str(obj), label=str(pred))\n",
    "\n",
    "        for node in nx_graph.nodes:\n",
    "            paths = [[node]]\n",
    "            for _ in range(iterations):\n",
    "                new_paths = []\n",
    "                for path in paths:\n",
    "                    if len(path) < depth:\n",
    "                        neighbors = list(nx_graph.neighbors(path[-1]))\n",
    "                        for neighbor in neighbors:\n",
    "                            new_paths.append(path + [neighbor])\n",
    "                paths.extend(new_paths)\n",
    "            walks.extend(paths)\n",
    "    return walks\n",
    "\n",
    "\n",
    "# 4. 使用 CBOW 生成嵌入\n",
    "def train_word2vec(walks, dimensions=200, window_size=5, sg=0, negative=25, epochs=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=walks,\n",
    "        vector_size=dimensions,\n",
    "        window=window_size,\n",
    "        sg=sg,\n",
    "        negative=negative,\n",
    "        epochs=epochs,\n",
    "        workers=4\n",
    "    )\n",
    "    for i in range(10):  # 进行 10 轮手动迭代\n",
    "        model.train(walks, total_examples=len(walks), epochs=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 3. 使用 Skip-gram 模型生成嵌入\n",
    "def train_skipgram_model(walks, dimensions=200, window_size=5, sg=1, negative=25, epochs=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=walks,\n",
    "        vector_size=dimensions,\n",
    "        window=window_size,\n",
    "        sg=sg,  # 使用 Skip-gram\n",
    "        negative=negative,\n",
    "        epochs=epochs,\n",
    "        workers=4\n",
    "    )\n",
    "    for i in range(10):  # 进行 10 轮手动迭代\n",
    "        model.train(walks, total_examples=len(walks), epochs=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 5. 特征提取\n",
    "def extract_features(model, nodes):\n",
    "    features = []\n",
    "    valid_nodes = []  # 用于存储成功提取特征的节点\n",
    "    for node in nodes:\n",
    "        if node in model.wv:\n",
    "            features.append(model.wv[node])\n",
    "            valid_nodes.append(node)\n",
    "    return features, valid_nodes\n",
    "\n",
    "\n",
    "# 6. 分类/回归任务\n",
    "def evaluate_features(features, labels, task=\"classification\"):\n",
    "    X = list(features.values())\n",
    "    y = [labels.get(node, 0) for node in features.keys()]\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "    accuracies = []\n",
    "    rmses = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        if task == \"classification\":\n",
    "            accuracies.append(accuracy_score(y_test, predictions))\n",
    "        else:\n",
    "            rmses.append(mean_squared_error(y_test, predictions, squared=False))\n",
    "\n",
    "    if task == \"classification\":\n",
    "        print(\"Average Accuracy:\", sum(accuracies) / len(accuracies))\n",
    "    else:\n",
    "        print(\"Average RMSE:\", sum(rmses) / len(rmses))\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:55:09.488630Z",
     "start_time": "2025-02-03T15:55:08.898313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "graphs = load_rdf_data()\n",
    "\n",
    "print(\"RDF 数据加载完成！\")"
   ],
   "id": "8106ac63c3f06b06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF 数据加载完成！\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:56:54.715343Z",
     "start_time": "2025-02-03T15:55:09.602907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import Graph, Namespace\n",
    "\n",
    "# # 结果展示\n",
    "# import pandas as pd\n",
    "# import graphs as tools\n",
    "#\n",
    "# affiliation_df = pd.DataFrame(affiliations, columns=[\"Subject\", \"Affiliation\"])\n",
    "# tools.display_dataframe_to_user(\"Affiliation Information\", affiliation_df)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# 固定随机数种子\n",
    "random.seed(42)\n",
    "\n",
    "graph = graphs[0]\n",
    "\n",
    "# 转换为三元组列表\n",
    "triples = list(graph)\n",
    "sampled_triples = random.sample(triples, 2500)\n",
    "sampled_graph = Graph()\n",
    "for triple in sampled_triples:\n",
    "    sampled_graph.add(triple)\n",
    "\n",
    "# 随机采样 100 个三元组\n",
    "sampled_graphs = [sampled_graph]\n",
    "# 定义命名空间（假设 affiliation 属于 swrc 命名空间）\n",
    "SWRC = Namespace(\"http://swrc.ontoware.org/ontology#\")\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# nodes = [str(node) for graph in graphs for node in graph.all_nodes()]\n",
    "for train_index, test_index in kf.split(sampled_triples):\n",
    "    train_graph_list, test_graph_list = [sampled_triples[i] for i in train_index], [sampled_triples[i] for i in test_index]\n",
    "    train_graph = Graph()\n",
    "\n",
    "    for triple in train_graph_list:\n",
    "        train_graph.add(triple)\n",
    "    train_graphs = [train_graph]\n",
    "\n",
    "    test_graph = Graph()\n",
    "\n",
    "    for triple in test_graph_list:\n",
    "        test_graph.add(triple)\n",
    "    test_graphs = [test_graph]\n",
    "\n",
    "    bfs_walks = generate_graph_walks(train_graphs, depth=8)\n",
    "    print(\"bfs_walks完成！\")\n",
    "\n",
    "\n",
    "    CBOW_embedding_model = train_word2vec(bfs_walks, dimensions=200, window_size=5, sg=0, negative=25)\n",
    "    print(\"嵌入模型训练完成！\")\n",
    "\n",
    "    affiliations = []\n",
    "    for s, p, o in train_graph:\n",
    "        if p == SWRC.affiliation:\n",
    "            affiliations.append([s, o])\n",
    "    X_train_nodes = [str(item[0]) for item in affiliations]\n",
    "    Y_train_nodes = [str(item[1]) for item in affiliations]\n",
    "\n",
    "    missing_X_nodes = [node for node in X_train_nodes if node not in CBOW_embedding_model.wv]\n",
    "    missing_Y_nodes = [node for node in Y_train_nodes if node not in CBOW_embedding_model.wv]\n",
    "\n",
    "    # 同步过滤有效的 X 和 Y 节点\n",
    "    filtered_affiliations = [\n",
    "        (x, y) for x, y in zip(X_train_nodes, Y_train_nodes)\n",
    "        if x in CBOW_embedding_model.wv and y in CBOW_embedding_model.wv\n",
    "    ]\n",
    "\n",
    "    # 分离过滤后的 X 和 Y\n",
    "    filtered_X_train_nodes = [x for x, y in filtered_affiliations]\n",
    "    filtered_Y_train_nodes = [y for x, y in filtered_affiliations]\n",
    "\n",
    "    # 提取特征\n",
    "    X_train_features, _ = extract_features(CBOW_embedding_model, filtered_X_train_nodes)\n",
    "    Y_train_features, _ = extract_features(CBOW_embedding_model, filtered_Y_train_nodes)\n",
    "\n",
    "    # 检查特征和标签是否一致\n",
    "    assert len(X_train_features) == len(filtered_Y_train_nodes), \"特征和标签数量仍然不一致！\"\n",
    "\n",
    "    affiliations = []\n",
    "    for s, p, o in test_graph:\n",
    "        if p == SWRC.affiliation:\n",
    "            affiliations.append([s, o])\n",
    "    X_test_nodes = [str(item[0]) for item in affiliations]\n",
    "    Y_test_nodes = [str(item[1]) for item in affiliations]\n",
    "\n",
    "    filtered_test_affiliations = [\n",
    "        (x, y) for x, y in zip(X_test_nodes, Y_test_nodes)\n",
    "        if x in CBOW_embedding_model.wv and y in CBOW_embedding_model.wv\n",
    "    ]\n",
    "\n",
    "    filtered_X_test_nodes = [x for x, y in filtered_test_affiliations]\n",
    "    filtered_Y_test_nodes = [y for x, y in filtered_test_affiliations]\n",
    "\n",
    "        # 检查是否存在 affiliation 数据\n",
    "    if not filtered_X_train_nodes or not filtered_X_test_nodes:\n",
    "        print(\"此折缺少 affiliation 数据，已跳过。\")\n",
    "        continue\n",
    "\n",
    "    X_test_features, _ = extract_features(CBOW_embedding_model, filtered_X_test_nodes)\n",
    "    Y_test_features, _ = extract_features(CBOW_embedding_model, filtered_Y_test_nodes)\n",
    "\n",
    "    # 检查一致性\n",
    "    assert len(X_test_features) == len(filtered_Y_test_nodes), \"测试集特征和标签数量不一致！\"\n",
    "\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    # 定义模型为 SVM\n",
    "    model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    model.fit(X_train_features, filtered_Y_train_nodes)\n",
    "\n",
    "    # 预测与评估\n",
    "    predictions = model.predict(X_test_features)\n",
    "    accuracy = accuracy_score(filtered_Y_test_nodes, predictions)\n",
    "\n",
    "    accuracies.append(accuracy_score(filtered_Y_test_nodes, predictions))\n",
    "\n",
    "    print(\"Average Accuracy:\", sum(accuracies) / len(accuracies))\n",
    "\n",
    "\n"
   ],
   "id": "8d4853493c554e58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "Average Accuracy: 1.0\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "Average Accuracy: 0.0\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "Average Accuracy: 1.0\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "此折缺少 affiliation 数据，已跳过。\n",
      "bfs_walks完成！\n",
      "嵌入模型训练完成！\n",
      "Average Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:56:54.736053Z",
     "start_time": "2025-02-03T15:56:54.734726Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ed3865e25fe34c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
